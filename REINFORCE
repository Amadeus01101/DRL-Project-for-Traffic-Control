"""REINFORCE AGENT IMPLEMENTATION"""


import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import gymnasium as gym


# Hyperparameters
gamma = 0.99  # Discount factor
lr = 0.01  # Learning rate
num_episodes = 2000 # nb of learning episodes

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, action_dim)

    def forward(self, state):
        x = F.relu(self.fc1(state))  # Activation function ReLU
        action_probs = F.softmax(self.fc2(x), dim=-1)  # Converts raw scores to a proba distribution over actions
        return action_probs


class ReinforceAgent:
    def __init__(self, env, gamma, lr):
        self.env = env
        self.gamma = gamma
        self.lr = lr

        self.state_dim = env.observation_space.shape[0]
        self.action_dim = env.action_space.n
        self.policy = PolicyNetwork(self.state_dim, self.action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

    def generate_episode(self, render = False):
        """
        Runs one full episode and stores (state, action, reward)
        """
        log_probs = [] # log of proba of chosen action
        rewards = [] # reward at each step of the episode
        state = self.env.reset()[0]  # begin new episode (gets initial state)
        done = False
        truncated = False

        while not (done or truncated): # runs loop until episode is done
            if render:
                self.env.render()  # to visalize the environment

            state = torch.tensor(state, dtype=torch.float32) # converts state into PyTorch tensor
            action_probs = self.policy(state) # passes state through agent's policy network
            action_dist = torch.distributions.Categorical(action_probs) # each action is asssigned a proba given by action_probs
            action = action_dist.sample() # randomly selects an action based on the probability distribution

            log_probs.append(action_dist.log_prob(action)) # store the log of that proba to then compute the gradient
            state, reward, done, truncated, _ = self.env.step(action.item()) # taking action in the actual environment
            rewards.append(reward) # store the associated reward

        return log_probs, rewards

    def compute_discounted_returns(self, rewards):
        """
        Computes discounted reward G for each time step.
        """
        G = []
        discounted_sum = 0
        for r in reversed(rewards):
            discounted_sum = r + self.gamma * discounted_sum
            G.insert(0, discounted_sum)  # insert at the beginning of the list

        G = torch.tensor(G, dtype=torch.float32)  # convert to PyTorch tensor
        return G

    def update_policy(self, log_probs, returns):
        """
        Computes the policy gradient loss and updates the policy network.
        """
        loss = 0 # total loss over the episode is initialized at 0
        # policy gradient theorem :
        for log_prob, G in zip(log_probs, returns):
            loss += -log_prob * G

        self.optimizer.zero_grad() # reset gradient to zero, clear out previously accumulated gradient by PyTorch
        loss.backward()  # compute gradients = backpropagation
        self.optimizer.step()  # update policy parameters using gradient descent


    def train(self, num_episodes, save_path="reinforce_agent.pth"):
        """
        Training loop for the REINFORCE algorithm.
        """
        best_reward = -float("inf")  # Track the best total reward

        for episode in range(num_episodes):
            log_probs, rewards = self.generate_episode()
            returns = self.compute_discounted_returns(rewards)
            self.update_policy(log_probs, returns)

            total_reward = sum(rewards)
            if episode % 10 == 0:
                print(f"Episode {episode}: Total Reward = {total_reward}")

            # Save the best performing model
            if total_reward > best_reward:
                best_reward = total_reward
                torch.save(self.policy.state_dict(), save_path)
                print(f"Model saved at Episode {episode} with Reward {best_reward}")

    def load_model(self, save_path="reinforce_agent.pth"):
        self.policy.load_state_dict(torch.load(save_path))
        print("Model loaded successfully!")


### TRAIN MODEL AND SAVE IT ### (Uncomment and run below to train stop when reward is at 10.000)

#env = gym.make("CartPole-v1", render_mode=None)  # Rend visible l'environnement
#env._max_episode_steps = 10000
#agent = ReinforceAgent(env, gamma=0.99, lr=0.01)  # Learning rate plus bas
#agent.train(num_episodes=100000)  # Entra√Ænement
#agent.generate_episode(render=False)  # Test avec visualisation

### VISUALIZE SAVED MODEL IN VIRTUAL ENVIRONMENT ### (Uncomment and run below to visualize the trained agent)

env = gym.make("CartPole-v1", render_mode="human")
agent = ReinforceAgent(env, gamma=0.99, lr=0.01)
agent.load_model("reinforce_agent.pth")  # Load trained model
agent.generate_episode(render=True)  # Run with visualization

env.close()
